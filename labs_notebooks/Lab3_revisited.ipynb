{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7cf7a2c",
   "metadata": {},
   "source": [
    "# Linear Regression Revisited\n",
    "This notebook revisits linear regression from first principles using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f27fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557df05",
   "metadata": {},
   "source": [
    "# Understanding Linear Regression\n",
    "We'll explore why we need linear regression and how to implement it from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0465b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3b91cb-f650-4c4c-a4a2-fc7b6572552f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c74230d1-e854-42e1-b856-a806a448e3e1",
   "metadata": {},
   "source": [
    "## Part 1 - using simple equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec96b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO get the boston dataset\n",
    "boston = fetch_openml(name=..., version=1, as_frame=True)\n",
    "df = ....frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebf547c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# @TODO plot LSTAT and MEDV variables\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(df[...], df[...])\n",
    "plt.xlabel('LSTAT (% lower status of the population)')\n",
    "plt.ylabel('MEDV (Median value of homes in $1000s)')\n",
    "plt.title('Boston Housing Dataset: LSTAT vs MEDV')\n",
    "# Highlight the first data point that we'll focus on\n",
    "# @TODO pick the first point from the dataset\n",
    "x_picked, y_picked = ..., ...\n",
    "plt.scatter(x_picked, y_picked, color=\"black\", s=100)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7c635",
   "metadata": {},
   "source": [
    "Linear regression helps us model the relationship between variables\n",
    "We can use it to predict house prices based on neighborhood characteristics\n",
    "It's also useful for understanding how variables like LSTAT impact house values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e452dd59",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# @TODO we are doing linear regression\n",
    "def neuron(x, w):\n",
    "    return ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce5ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO add random weights\n",
    "w = np.array([...,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b2dd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO assign correct values, use x from dataset\n",
    "x_1 = ...\n",
    "x_b = np....(x_1)\n",
    "x = np....([x_1, x_b]).T\n",
    "y_pred = neuron(x, w)\n",
    "error = (df[\"MEDV\"].values - y_pred)\n",
    "mse = np.mean(error**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec94ffc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "# @TODO plot the boston data\n",
    "plt.scatter(..., ...)\n",
    "# @TODO plot predicted line\n",
    "plt.plot(..., ..., c='r', label='Initial Line')\n",
    "# @TODO plot picked point in black\n",
    "plt.scatter(..., ..., color=..., s=100, label='Focus Point')\n",
    "plt.title(f'MSE: {mse:.2f}')\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('MEDV')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a0425",
   "metadata": {},
   "source": [
    "The line doesn't fit our focus point well yet\n",
    "We need a systematic way to find weights that make the line pass through specific points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd17cc8",
   "metadata": {},
   "source": [
    "Initial random weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f0202",
   "metadata": {},
   "source": [
    "The correct_weights function calculates weights to make a line pass through a point (x_n, y_n).\n",
    "\n",
    "For a linear model with equation: y = w_0*x + w_1\n",
    "We want to find weights such that: y_n = w_0*x_n + w_1\n",
    "\n",
    "Using LaTeX notation:\n",
    "$y_n = w_0 \\cdot x_n + w_1$\n",
    "\n",
    "To find w_0, we rearrange:\n",
    "$w_0 = ...TODO$\n",
    "\n",
    "This is exactly what our function calculates:\n",
    "w_0 = ...TODO\n",
    "\n",
    "For w_1, we use our new w_0 value:\n",
    "$w_1 = ...TODO$\n",
    "\n",
    "This ensures that our line equation y = w_0*x + w_1 will pass through the point (x_n, y_n).\n",
    "Since we have 2 unknowns (w_0 and w_1) but only 1 constraint (the line must pass through one point),\n",
    "there are infinitely many solutions. Our approach gives one particular solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b666715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO fill in the function\n",
    "def correct_weights(x_n, y_n, w):\n",
    "    w_0 = ...\n",
    "    w_1 = ...\n",
    "    return w_0, w_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d0c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify this works with our focus point\n",
    "x_n = df['LSTAT'][0]  # x-value of our focus point\n",
    "y_n = df['MEDV'][0]   # y-value of our focus point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e5a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n, y_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac91ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weights that make our line pass through the focus point\n",
    "w_0, w_1 = correct_weights(x_n, y_n, w)\n",
    "exact_weights = np.array([w_0, w_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2338af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Focus point: ({x_n}, {y_n})\")\n",
    "print(f\"Original weights: {w}\")\n",
    "print(f\"Exact weights: {exact_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e9bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with new weights\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(df['LSTAT'], df['MEDV'])\n",
    "\n",
    "\n",
    "# Plot line with exact weights\n",
    "# @TODO\n",
    "x_with_bias = np.vstack([..., np.ones_like(...)]).T\n",
    "# @TODO fill in the weidhts that we have just calculated\n",
    "y_pred_exact = neuron(x_with_bias, ...)\n",
    "plt.plot(x_1, y_pred_exact, 'g-', label='Exact Line')\n",
    "plt.scatter(x_n, y_n, color=\"black\", s=100, label='Focus Point')\n",
    "plt.title('Line passing through focus point')\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('MEDV')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3250e40-adc9-4443-a6d3-ac338a0e5f0f",
   "metadata": {},
   "source": [
    "## Part 2 - Applying the equation iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0bd1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with our random weights\n",
    "current_weights = w.copy()\n",
    "alpha = 0.2  # Learning rate - fraction of correction to apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eb7ab2-3662-4de4-87be-131b4f2334fa",
   "metadata": {},
   "source": [
    "Now lets implement a way to not update the weights straigh away, do it iteratively - this will be useful in next section. In the plot below you should see how the line slowly approaches our \"ideal\" weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34881493",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "# Iteratively adjust weights\n",
    "fig, axe = plt.subplots(3, 3, figsize=(12, 8))\n",
    "\n",
    "for i in range(9):\n",
    "    # @TODO Calculate the correct weights for picked point, based on current_weights\n",
    "    w_0_ideal, w_1_ideal = ...(..., ..., ...)\n",
    "    ideal_weights = np.array([w_0_ideal, w_1_ideal])\n",
    "    \n",
    "    # @TODO Calculate the difference between ideal_weights and current weights\n",
    "    weight_diff = ideal_weights - ...\n",
    "    # @TODO update current weights based on the fraction of the weight difference\n",
    "    current_weights = ... + alpha * weight_diff\n",
    "    \n",
    "    y_pred = neuron(x, current_weights)\n",
    "    axe[i//3, i%3].scatter(df['LSTAT'], df['MEDV'], alpha=0.5)\n",
    "    axe[i//3, i%3].plot(x_1, y_pred_init, 'r--', alpha=0.5, label='Initial Line')\n",
    "    axe[i//3, i%3].plot(x_1, y_pred, 'g-', label=f'Iteration {i+1}')\n",
    "    axe[i//3, i%3].set_title(f'Iteration {i+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92aa9a0",
   "metadata": {},
   "source": [
    "# Regression to multiple points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4662734",
   "metadata": {},
   "source": [
    "## Part 3 - Simple regression to all the points\n",
    "\n",
    "Now we would like to actually do the regression but to all of the points.\n",
    "We will use MSE to be metric for our method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2586f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with our random weights\n",
    "current_weights = w.copy()\n",
    "alpha = 0.02  # Learning rate - fraction of correction to apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d50c8d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Iteratively adjust weights\n",
    "fig, axe = plt.subplots(3, 3, figsize=(12, 8))\n",
    "current_weights = w.copy()\n",
    "mse_history = []\n",
    "for i in range(9):\n",
    "    for j in range(len(df[\"LSTAT\"])):\n",
    "        # @TODO each iteration use next point\n",
    "        x_n = df[...][...]\n",
    "        y_n = df[...][...]\n",
    "        # @TODO calculate ideal weights\n",
    "        w_0_ideal, w_1_ideal = ...(..., ..., ...)\n",
    "        ideal_weights = np.array([w_0_ideal, w_1_ideal])\n",
    "        # @TODO Calculate the difference between ideal_weights and current weights\n",
    "        weight_diff = ideal_weights - ...\n",
    "        # @TODO update current weights based on the fraction of the weight difference\n",
    "        current_weights = ... + alpha * weight_diff\n",
    "\n",
    "        # @TODO calculate prediction\n",
    "        y_pred = neuron(x, ...)\n",
    "        error = df[\"MEDV\"].values - y_pred\n",
    "        mse = np.mean(error**2)\n",
    "        mse_history.append(mse)\n",
    "    \n",
    "    y_pred = neuron(x, current_weights)\n",
    "    axe[i//3, i%3].scatter(df['LSTAT'], df['MEDV'], alpha=0.5)\n",
    "    axe[i//3, i%3].plot(x_1, y_pred_init, 'r--', alpha=0.5, label='Initial Line')\n",
    "    axe[i//3, i%3].plot(x_1, y_pred, 'g-', label=f'Iteration {i+1}')\n",
    "    axe[i//3, i%3].scatter(x_n, y_n, color=\"black\", s=80, label='Focus Point')\n",
    "    axe[i//3, i%3].set_title(f'Iteration {i+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc88ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "# @TODO plot mse history\n",
    "plt.plot(..., 'ro-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.title('MSE over Iterations - Instability in Single-Point Updates')\n",
    "plt.grid(True)\n",
    "\n",
    "# The MSE history shows instability as we're updating weights after each point\n",
    "# This is because each point pulls the weights in a different direction\n",
    "# The line oscillates between trying to fit different points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c927de8",
   "metadata": {},
   "source": [
    "## Part 4 - Batched regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803d3135-d351-4f48-bd11-c47c291f2148",
   "metadata": {},
   "source": [
    "The calculation of each point may be unstable, so we can use batches to stabilise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2414e0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# Implementing batch gradient descent for more stability\n",
    "current_weights = w.copy()\n",
    "alpha = 0.03  # Smaller learning rate for batch updates\n",
    "epochs = 360\n",
    "batch_mse_history = []\n",
    "\n",
    "# Create subplots for batch gradient descent visualization\n",
    "fig, axe = plt.subplots(3, 3, figsize=(12, 8))\n",
    "\n",
    "# Reset weights for visualization\n",
    "current_weights = w.copy()\n",
    "y_pred_init = neuron(x, current_weights)\n",
    "\n",
    "# Always include first and last epoch\n",
    "# For the remaining 7 plots, select epochs based on modulo\n",
    "plot_count = 0\n",
    "\n",
    "# Track lines for several epochs\n",
    "for epoch in range(epochs):\n",
    "    # Calculate weight adjustments based on all points\n",
    "    total_adjustment = np.zeros(2)\n",
    "    for j in range(len(df[\"LSTAT\"])):\n",
    "        x_n = df['LSTAT'][j]\n",
    "        y_n = df['MEDV'][j]\n",
    "        w_0_ideal, w_1_ideal = correct_weights(x_n, y_n, current_weights)\n",
    "        ideal_weights = np.array([w_0_ideal, w_1_ideal])\n",
    "        \n",
    "        # @TODO accumulate weight adjustments\n",
    "        adjustment = ideal_weights - ...\n",
    "        total_adjustment += ...\n",
    "    \n",
    "    # @TODO pply average adjustment\n",
    "    avg_adjustment = ... / ...\n",
    "    current_weights = current_weights + ... * avg_adjustment\n",
    "    \n",
    "    # Calculate MSE with new weights\n",
    "    y_pred = neuron(x, current_weights)\n",
    "    error = df[\"MEDV\"].values - y_pred\n",
    "    mse = np.mean(error**2)\n",
    "    batch_mse_history.append(mse)\n",
    "    \n",
    "    # Plot the line at specific epochs to see progression\n",
    "    # Plot first epoch, last epoch, and 7 epochs in between using modulo\n",
    "    if epoch == 0 or epoch == epochs-1 or epoch % (epochs // 8) == 0:\n",
    "        if plot_count < 9:  # Only plot if we have space in our 3x3 grid\n",
    "            axe[plot_count//3, plot_count%3].scatter(df['LSTAT'], df['MEDV'], alpha=0.5)\n",
    "            axe[plot_count//3, plot_count%3].plot(x_1, y_pred_init, 'r--', alpha=0.5, label='Initial Line')\n",
    "            axe[plot_count//3, plot_count%3].plot(x_1, y_pred, 'g-', label=f'Epoch {epoch+1}')\n",
    "            axe[plot_count//3, plot_count%3].set_title(f'Epoch {epoch+1}')\n",
    "            axe[plot_count//3, plot_count%3].grid(True)\n",
    "            plot_count += 1\n",
    "    \n",
    "\n",
    "# Add a title to the entire figure of subplots\n",
    "fig.suptitle('Batch Gradient Descent - Line Evolution', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout for the suptitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2eb08e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Plot MSE history for batch updates\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, epochs + 1), batch_mse_history, 'go-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.title('MSE over Epochs - Batch Updates')\n",
    "plt.grid(True)\n",
    "\n",
    "# Batch gradient descent produces more stable convergence\n",
    "# By averaging updates across all points, we get a more balanced direction\n",
    "# The line gradually fits the overall trend in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7401f8eb",
   "metadata": {},
   "source": [
    "We can see that the optimalisation is much more stable but still not good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380b5f23",
   "metadata": {},
   "source": [
    "## Part 5 - MSE optimisation\n",
    "\n",
    "So we have been using mean squared error as a metric, but now we should optimise for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c584dd",
   "metadata": {},
   "source": [
    "Mean Squared Error (MSE) is defined as the average of squared differences between predictions and actual values\n",
    "Given:\n",
    "- $X$ is our input matrix with features (LSTAT) and bias term\n",
    "- $w$ is our weight vector (slope and intercept)\n",
    "- $y$ is our target values (MEDV)\n",
    "- $\\hat{y} = Xw$ is our prediction\n",
    "\n",
    "The MSE is calculated as:\n",
    "$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - X_i w)^2$\n",
    "\n",
    "To optimize MSE, we need its gradient with respect to weights $w$:\n",
    "$\\nabla_w \\text{MSE} = \\frac{\\partial \\text{MSE}}{\\partial w}$\n",
    "\n",
    "Expanding the differentiation:\n",
    "$\\nabla_w \\text{MSE} = \\frac{\\partial}{\\partial w}\\frac{1}{n}\\sum_{i=1}^{n}(y_i - X_i w)^2$\n",
    "\n",
    "Solving step by step:\n",
    "$\\nabla_w \\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\partial}{\\partial w}(y_i - X_i w)^2$\n",
    "\n",
    "Using the chain rule: $\\frac{\\partial}{\\partial w}(y_i - X_i w)^2 = 2(y_i - X_i w)\\frac{\\partial}{\\partial w}(y_i - X_i w)$\n",
    "\n",
    "Since $\\frac{\\partial}{\\partial w}(y_i - X_i w) = -X_i^T$, we get:\n",
    "$\\nabla_w \\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}2(y_i - X_i w)(-X_i^T) = -\\frac{2}{n}\\sum_{i=1}^{n}X_i^T(y_i - X_i w)$\n",
    "\n",
    "In vector form for all samples:\n",
    "$\\nabla_w \\text{MSE} = -\\frac{2}{n}X^T(y - Xw) = \\frac{2}{n}X^T(Xw - y)$\n",
    "\n",
    "For gradient descent, we update weights in the opposite direction of the gradient:\n",
    "$w_{new} = w_{old} - \\alpha \\nabla_w \\text{MSE}$\n",
    "\n",
    "Where $\\alpha$ is our learning rate. Plugging in the gradient:\n",
    "$w_{new} = w_{old} - \\alpha \\frac{2}{n}X^T(Xw_{old} - y)$\n",
    "\n",
    "This is our update rule for weights using gradient descent to minimize MSE.\n",
    "\n",
    "[Linear regression article](https://medium.com/@gurjinderkaur95/linear-regression-and-gradient-descent-in-python-937e77ec68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddffa9f6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Initialize weights and parameters\n",
    "current_weights = w.copy()\n",
    "learning_rate = 0.0001  # Small learning rate for MSE-based updates\n",
    "epochs = 100\n",
    "mse_history = []\n",
    "\n",
    "# Create subplots for MSE-based gradient descent visualization\n",
    "fig, axs = plt.subplots(3, 3, figsize=(12, 8))\n",
    "\n",
    "# Initial prediction\n",
    "y_pred_init = neuron(x, current_weights)\n",
    "\n",
    "# Convert data to numpy arrays for efficient computation\n",
    "X = np.vstack([df['LSTAT'], np.ones(len(df['LSTAT']))]).T  # Add bias term\n",
    "y = df['MEDV'].values\n",
    "\n",
    "# Track progress for plotting\n",
    "plot_count = 0\n",
    "\n",
    "# Run gradient descent with MSE-based updates\n",
    "for epoch in range(epochs):\n",
    "    # Compute predictions for all points (just for MSE calculation)\n",
    "    y_pred = neuron(X, current_weights)\n",
    "    \n",
    "    # Compute and record MSE\n",
    "    error = y - y_pred\n",
    "    mse = np.mean(error**2)\n",
    "    mse_history.append(mse)\n",
    "    \n",
    "    # Stochastic updates - one point at a time\n",
    "    for j in range(len(df[\"LSTAT\"])):\n",
    "        # Select a single data point\n",
    "        x_j = np.array([df['LSTAT'][j], 1]).reshape(1, 2)  # Single point with bias\n",
    "        y_j = df['MEDV'][j]  # Single target\n",
    "        \n",
    "        # Compute prediction for this point\n",
    "        y_pred_j = neuron(x_j, current_weights)\n",
    "        \n",
    "        # @TODO compute gradient for this single point\n",
    "        gradient_j = ...\n",
    "        \n",
    "        # @TODO update weights immediately after seeing this point\n",
    "        current_weights = current_weights - ...\n",
    "    \n",
    "    # Plot at specific epochs to see progression\n",
    "    if epoch == 0 or epoch == epochs-1 or epoch % (epochs // 8) == 0:\n",
    "        if plot_count < 9:  # Only plot if we have space in our 3x3 grid\n",
    "            row, col = plot_count // 3, plot_count % 3\n",
    "            axs[row, col].scatter(df['LSTAT'], df['MEDV'], alpha=0.5)\n",
    "            axs[row, col].plot(df['LSTAT'], y_pred_init, 'r--', alpha=0.5, label='Initial')\n",
    "            axs[row, col].plot(df['LSTAT'], y_pred, 'b-', label=f'Epoch {epoch+1}')\n",
    "            axs[row, col].set_title(f'MSE: {mse:.2f} (Epoch {epoch+1})')\n",
    "            axs[row, col].grid(True)\n",
    "            axs[row, col].set_xlabel('LSTAT')\n",
    "            axs[row, col].set_ylabel('MEDV')\n",
    "            plot_count += 1\n",
    "\n",
    "# Add a title to the entire figure\n",
    "fig.suptitle('Gradient Descent with MSE Optimization', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout for the suptitle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f99393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MSE history for batch updates\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, epochs + 1), mse_history, 'go-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.title('MSE over Epochs - Batch Updates')\n",
    "plt.grid(True)\n",
    "\n",
    "# Batch gradient descent produces more stable convergence\n",
    "# By averaging updates across all points, we get a more balanced direction\n",
    "# The line gradually fits the overall trend in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931914e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Final comparison plot after optimization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['LSTAT'], df['MEDV'], alpha=0.7, label='Data Points')\n",
    "plt.plot(df['LSTAT'], y_pred, 'b-', linewidth=2, label='Optimized Line (MSE Gradient Descent)')\n",
    "\n",
    "\n",
    "plt.xlabel('LSTAT (% lower status of the population)', fontsize=12)\n",
    "plt.ylabel('MEDV (Median value of homes in $1000s)', fontsize=12)\n",
    "plt.title('Linear Regression: Before and After MSE Optimization', fontsize=14)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e808cdf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576a340f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
